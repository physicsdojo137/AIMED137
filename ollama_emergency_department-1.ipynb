{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!sudo apt update"
      ],
      "metadata": {
        "id": "bQ13C5pzWGus",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install -y pciutils"
      ],
      "metadata": {
        "id": "VC3eTWW4WXY4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "id": "42oBVh11WtuV",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "def run_ollama_serve():\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "thread = threading.Thread(target=run_ollama_serve)\n",
        "\n",
        "thread.start()\n",
        "time.sleep(5)  # Allows service to initialize\n"
      ],
      "metadata": {
        "id": "PS06EZbjYl32"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull gemma3:12b\n",
        "!ollama pull gemma3:1b\n",
        "!ollama pull deepseek-r1:14b"
      ],
      "metadata": {
        "id": "m_B5B9iBZEbo",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama list"
      ],
      "metadata": {
        "id": "o8j80gMOZaiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-ollama\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_ollama.llms import OllamaLLM\n",
        "from IPython.display import Markdown\n"
      ],
      "metadata": {
        "id": "J07C8IO1bIav",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#template = \"\"\"Question: {question}\n",
        "\n",
        "#Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "template = \"\"\"As a medical consultant, you will respond to questions about various medical scenarios. This is a multiple-choice question with up to 10 possibilities. Select the answer that is most likely and estimate your degree of confidence from 1-5, where 5 is very confident. Report the correct answer A-J.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "model = OllamaLLM(model=\"gemma3:12b\")\n",
        "\n",
        "\n",
        "chain = prompt | model\n",
        "user_input = input(\"Enter question with multiple choice answers: \")\n",
        "display(Markdown(chain.invoke({\"question\": user_input})))"
      ],
      "metadata": {
        "id": "vMoV6VmNrh9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#template = \"\"\"Question: {question}\n",
        "\n",
        "#Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "template = \"\"\"As a medical consultant, you will respond to questions about various medical scenarios. This is a multiple-choice question with up to 10 possibilities. Select the answer that is most likely and estimate your degree of confidence from 1-5, where 5 is very confident. Report the correct answer A-J.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "model = OllamaLLM(model=\"gemma3:1b\")\n",
        "\n",
        "\n",
        "chain = prompt | model\n",
        "user_input = input(\"Enter question with multiple choice answers: \")\n",
        "display(Markdown(chain.invoke({\"question\": user_input})))"
      ],
      "metadata": {
        "id": "88V8vqEdI7B5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#template = \"\"\"Question: {question}\n",
        "\n",
        "#Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "template = \"\"\"As a medical consultant, you will respond to questions about various medical scenarios. This is a multiple-choice question with up to 10 possibilities. Select the answer that is most likely and estimate your degree of confidence from 1-5, where 5 is very confident. Report the correct answer A-J.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "model = OllamaLLM(model=\"deepseek-r1:14b\")\n",
        "\n",
        "\n",
        "chain = prompt | model\n",
        "user_input = input(\"Enter question with multiple choice answers: \")\n",
        "display(Markdown(chain.invoke({\"question\": user_input})))"
      ],
      "metadata": {
        "id": "vs2QIJIUrhi2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}