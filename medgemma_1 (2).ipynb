{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fgVWTMK9SNz"
      },
      "source": [
        "\n",
        "# MedGemma: Google Model - hosted on Hugging Face\n",
        "Modified from Notebook: https://github.com/Google-Health/medgemma/blob/main/notebooks/quick_start_with_hugging_face.ipynb\n",
        "\n",
        "\n",
        "\n",
        "Tutorial: MedGemma using a chest xay stored on your local google drive. Once you can use this simple notebook, play with different models, images, system prompts, and user prompts.\n",
        "\n",
        "Learn more about the model at the [HAI-DEF developer site](https://developers.google.com/health-ai-developer-foundations/medgemma)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Note: Make sure you have HF_Token from huggingface so you can access all the models. Put the HF_Token into google secrets on the left - see the key."
      ],
      "metadata": {
        "id": "o5-F0pGPasUO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwUUIY0gpY4W"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "#google_colab = \"google.colab\" in sys.modules and not os.environ.get(\"VERTEX_PRODUCT\")\n",
        "\n",
        "from google.colab import userdata\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mount drive google colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "DS7hwNZI6j3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7xTbWg6pY4W"
      },
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CulOXOrhpY4W"
      },
      "outputs": [],
      "source": [
        "! pip install --upgrade --quiet accelerate bitsandbytes transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRN9Yg_kpY4X"
      },
      "source": [
        "## Set model variant and configuration\n",
        "Note you may need to go to the model page on hugging face for google/medgemma-4b-it to get permission to use the model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IT4FkbLaaTcS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YORs_sDfpY4X"
      },
      "outputs": [],
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "model_id = f\"google/medgemma-4b-it\"\n",
        "\n",
        "model_kwargs = dict(\n",
        "  torch_dtype=torch.bfloat16,\n",
        "  device_map=\"auto\",\n",
        ")\n",
        "model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(load_in_4bit=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-4LriNOpY4X"
      },
      "source": [
        "**Specify image and text inputs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UterxS4WpY4X"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from IPython.display import Image as IPImage, display, Markdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSTA4gRzpY4X"
      },
      "source": [
        "**Run model with the `pipeline` API**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARXpCL8cpY4X"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"image-text-to-text\",\n",
        "    model=model_id,\n",
        "    model_kwargs=model_kwargs,\n",
        ")\n",
        "\n",
        "pipe.model.generation_config.do_sample = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Core Question/Answer\n",
        "1. Get image\n",
        "2. Set up system instructions\n",
        "3. Create a question- the prompt"
      ],
      "metadata": {
        "id": "B3czPEjFU5ZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#set image to an image on my google drive\n",
        "\n",
        "image_path_in_drive = '/content/drive/MyDrive/med-gemma-colab/effusion1.jpeg' # @param {\"type\":\"string\"}\n",
        "image_filename = os.path.basename(image_path_in_drive)\n",
        "# Load the image using PIL\n",
        "image = Image.open(image_path_in_drive)\n",
        "\n",
        "\n",
        "system_instruction = \"You are a Senior Emergency Physician Attending.\" # @param {\"type\":\"string\"}\n",
        "prompt = \"Describe this chest xray- patient was in an MVA with chest trauma\" # @param {\"type\":\"string\"}\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": [{\"type\": \"text\", \"text\": system_instruction}]\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"text\", \"text\": prompt},\n",
        "            {\"type\": \"image\", \"image\": image}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n"
      ],
      "metadata": {
        "id": "pcINDgjuIcGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Find the Answer using your chosen model"
      ],
      "metadata": {
        "id": "o_Pb_XOkV9Jm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = pipe(text=messages, max_new_tokens=300)\n",
        "response = output[0][\"generated_text\"][-1][\"content\"]\n",
        "\n",
        "display(Markdown(f\"\\n\\n---\\n\\n**[ User ]**\\n\\n{prompt}\"))\n",
        "display(IPImage(filename=image_path_in_drive, height=300))\n",
        "display(Markdown(f\"\\n\\n---\\n\\n**[ MedGemma ]**\\n\\n{response}\\n\\n---\\n\\n\"))"
      ],
      "metadata": {
        "id": "6UP0vmElIrsP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}